{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60fb483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importations\n",
    "import requests\n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "from langdetect import detect\n",
    "import os\n",
    "import glob\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23c171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(chemin,contenu):\n",
    "    with open(chemin,\"w\",encoding=\"utf-8\") as w :\n",
    "        w.write(json.dumps(contenu, indent=2, ensure_ascii=False))\n",
    "def ouvrir_json(chemin) :\n",
    "    with open(chemin,encoding=\"utf-8\") as f :\n",
    "        toto = json.load(f)\n",
    "    return toto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3918076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requete_base(lang) : \n",
    "    nom = \"requetes_chansons_\"+lang+\".json\"\n",
    "    if os.path.exists(nom) :\n",
    "        return nom\n",
    "    langues = {\"fr\":\"Q150\",\"en\" :\"Q1860\" }\n",
    "    #récupérer aussi l'auteur ?\n",
    "    requete_all = \"\"\"\n",
    "    SELECT DISTINCT ?chanson ?chansonLabel ?performer ?performerLabel WHERE\n",
    "    {\n",
    "      ?chanson wdt:P7937 wd:Q7366;\n",
    "               wdt:P175 ?performer;\n",
    "               rdfs:label ?chansonLabel;\n",
    "               wdt:P407 wd:%s.\n",
    "       ?performer rdfs:label ?performerLabel.\n",
    "      FILTER(lang(?chansonLabel)=\"%s\").\n",
    "      FILTER(lang(?performerLabel)=\"%s\").\n",
    "  }\n",
    "    \"\"\"%(langues[lang],lang,lang)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = requests.get(url, params={'query': requete_all, 'format': 'json'}).json()\n",
    "    \n",
    "    write_json(nom,data)\n",
    "    return nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc7efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structuration_requete(fichier) :\n",
    "\n",
    "    resultats_json = ouvrir_json(fichier)\n",
    "    liste_niv1 = resultats_json[\"results\"][\"bindings\"]\n",
    "    \n",
    "    resultats = {}\n",
    "    chanteur = {}\n",
    "   \n",
    "    for un in liste_niv1 :\n",
    "        if un['chanson']['value'] not in resultats :\n",
    "            k = un['chanson']['value'].split(\"http://www.wikidata.org/entity/\")[1]\n",
    "            resultats[k] = (un['chansonLabel']['value'],un['performerLabel']['value'])\n",
    "    \"\"\"        \n",
    "    for un in liste_niv1 :\n",
    "        if un['performerLabel']['value'] not in chanteur :\n",
    "            chanteur[un['performerLabel']['value']] = [un['chansonLabel']['value']]\n",
    "        else :\n",
    "            chanteur[un['performerLabel']['value']].append(un['chansonLabel']['value'])\n",
    "   \"\"\"     \n",
    "    return resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc77ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/44468300/how-to-pos-tag-a-french-sentence\n",
    "def etiquetage(titre):\n",
    "    \n",
    "    #l'apostrophe crée des bugs lors de la requête sparql\n",
    "    titre = re.sub(\"'\",\" \",titre)\n",
    "    \n",
    "    #à mettre à jour selon la machine\n",
    "    jar = 'C:/Users/phisi/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar'\n",
    "    #à mettre à jour selon la machine\n",
    "    model = 'C:/Users/phisi/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/models/french-ud.tagger'\n",
    "    #à mettre à jour selon la machine\n",
    "    java_path = \"C:/Program Files/Java/jdk-18.0.1.1/bin/java.exe\"\n",
    "    \n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "    \n",
    "    etiquetages = pos_tagger.tag(titre.split())\n",
    "    \n",
    "    return etiquetages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6c579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fichier du glaff lourd (pas présent dans le répertoire ressources)\n",
    "#il est disponible à l'adresse http://redac.univ-tlse2.fr/lexiques/glaff.html\n",
    "def lemmatisation(fichier=\"ressources/glaff-1.2.2.txt\") :\n",
    "\n",
    "    f = open(fichier, encoding=\"utf-8\")\n",
    "    dico_lemmes = {}\n",
    "\n",
    "    ligne = f.readline()\n",
    "    while len(ligne)>1:\n",
    "\n",
    "        #on recupère les différents éléments du GLAFF\n",
    "        elems = re.split(\"\\|\", re.sub(\"\\n\", \"\", ligne))\n",
    "        \n",
    "        forme = elems[0]\n",
    "        lemme = elems[2]\n",
    "       \n",
    "        #on considere que la premiere forme est la plus populaire (à vérifier)\n",
    "        if forme not in dico_lemmes : \n",
    "            dico_lemmes[forme] = lemme\n",
    "        ligne = f.readline()\n",
    "    f.close()\n",
    "    return dico_lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e608915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexeme_jdm(lang,dico_reponses,dico_lemmes) :\n",
    "    dico_langues = {'fr':'french','en':'english'}\n",
    "    cle = random.choice(list(dico_reponses.keys()))\n",
    "    reponse = dico_reponses[cle][0]\n",
    "    \n",
    "    print(\"Reponse : \",reponse)\n",
    "   \n",
    "    dico_jdm = {}\n",
    "    rep_tagged = etiquetage(reponse)\n",
    "    print(\"Etiquetage de la reponse :\",rep_tagged)\n",
    "    #print(rep_tagged)\n",
    "    for r in rep_tagged : \n",
    "        if r[0] in stopwords.words(dico_langues[lang]) :\n",
    "            dico_jdm[r[0]] = {r[0]}\n",
    "        #on ne modifie pas les noms propres\n",
    "        elif r[1] == 'PROPN' or r[1] == 'NUM':\n",
    "            dico_jdm[r[0]] = {r[0]}\n",
    "        else :\n",
    "            #traitement de chaque token (lemme)\n",
    "            mot = r[0].lower()\n",
    "            if dico_lemmes.get(mot) != None :\n",
    "                lemme = dico_lemmes[mot]\n",
    "            else :\n",
    "                lemme = mot\n",
    "            requete_all = \"\"\"\n",
    "            PREFIX lexvo: <http://lexvo.org/id/iso639-3/>\n",
    "            PREFIX dbnary: <http://kaiko.getalp.org/dbnary#>\n",
    "            PREFIX lemon: <http://lemon-model.net/lemon#>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "            select distinct ?lex ?syn ?ant{\n",
    "\n",
    "                ?lexeme ontolex:canonicalForm ?f.\n",
    "                ?f ontolex:writtenRep '%s'@%s.\n",
    "                ?lexeme rdfs:label ?lex.\n",
    "\n",
    "                OPTIONAL {\n",
    "                    ?lexeme dbnary:synonym ?synonym.\n",
    "                    ?synonym dbnary:describes ?synClass.\n",
    "                    ?synClass rdfs:label ?syn.\n",
    "                    FILTER(lang(?syn) = \"%s\")\n",
    "                }\n",
    "\n",
    "                OPTIONAL {\n",
    "                    ?lexeme dbnary:antonym ?antonym.\n",
    "                    ?antonym dbnary:describes ?antClass.\n",
    "                    ?antClass rdfs:label ?ant.\n",
    "                    FILTER(lang(?ant) = \"%s\")\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "            \"\"\"%(lemme,lang,lang,lang)\n",
    "            url = 'http://kaiko.getalp.org/sparql'\n",
    "            data = requests.get(url, params={'query': requete_all, 'format': 'json'}).json()\n",
    "            \n",
    "            #si un mot n'a pas de synonymes / antonymes, on le note pour pouvoir ensuite exclure ce choix de reponse\n",
    "            if len(data['results']['bindings']) == 0 :\n",
    "                dico_jdm[r[0]] = {0}\n",
    "            else :     \n",
    "                i = 0\n",
    "                for d in data['results']['bindings'] :\n",
    "                    if i == 0 :\n",
    "                        cle_token = d['lex']['value']\n",
    "                        #print(cle_token)\n",
    "                        dico_jdm[cle_token] = {}\n",
    "                        dico_jdm[cle_token]['syn'] = set()\n",
    "                        dico_jdm[cle_token]['ant'] = set()\n",
    "                        i+=1\n",
    "                    for k,v in d.items() :\n",
    "                        if k != 'lex' and v['xml:lang'] == lang:\n",
    "                            dico_jdm[cle_token][k].add(v['value'])\n",
    "    print(\"-\"*70)\n",
    "    print(\"Résultat : \")                   \n",
    "    return dico_jdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af267f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_lemmes = lemmatisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b47c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse :  À mourir pour mourir\n",
      "Etiquetage de la reponse : [('À', 'ADP'), ('mourir', 'VERB'), ('pour', 'ADP'), ('mourir', 'VERB')]\n",
      "----------------------------------------------------------------------\n",
      "Résultat : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'à': {'syn': set(), 'ant': set()},\n",
       " 'mourir': {'syn': {'avaler sa chique',\n",
       "   'avaler son bulletin de naissance',\n",
       "   'avaler son extrait de naissance',\n",
       "   'avoir fait son temps',\n",
       "   'calancher',\n",
       "   'camarder',\n",
       "   'canner',\n",
       "   'casser sa pipe',\n",
       "   'claboter',\n",
       "   'clamser',\n",
       "   'clapser',\n",
       "   'claquer',\n",
       "   'consumer',\n",
       "   'crever',\n",
       "   'dernière demeure',\n",
       "   'disparaître',\n",
       "   'décéder',\n",
       "   'défuncter',\n",
       "   'défunter',\n",
       "   'expirer',\n",
       "   'fermer les yeux',\n",
       "   'finir',\n",
       "   'lever les fourches',\n",
       "   'monter au ciel',\n",
       "   'partir',\n",
       "   'partir aux fleurs',\n",
       "   'passer de vie à trépas',\n",
       "   'passer l’arme à gauche',\n",
       "   'payer sa dette à la nature',\n",
       "   'perdre la vie',\n",
       "   'périr',\n",
       "   'rendre le dernier soupir',\n",
       "   'rendre l’âme',\n",
       "   'rendre son âme à Dieu',\n",
       "   'succomber',\n",
       "   'trépasser',\n",
       "   'éteindre',\n",
       "   'être rappelé à Dieu'},\n",
       "  'ant': {'démourir',\n",
       "   'naître',\n",
       "   'renaître',\n",
       "   'ressusciter',\n",
       "   'revivre',\n",
       "   'se réincarner',\n",
       "   'survivre',\n",
       "   'vivre'}},\n",
       " 'pour': {'pour'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico_songs= structuration_requete('ressources/requetes_chansons_fr.json')\n",
    "lexeme_jdm(\"fr\",dico_songs,dico_lemmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bef0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
